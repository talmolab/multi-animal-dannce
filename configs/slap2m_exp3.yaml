### basic ###
# File name of io.yaml file, which should exist in the directory from which you call dannce functions
io_config: io.yaml
random_seed: 1024

### data ###
dataset: slap2m # dataset type, options are 'label3d', 'rat7m', 'pair'
camnames: ['back', 'backL', 'mid', 'midL', 'top', 'topL', 'side', 'sideL'] 
n_views: 8
num_validation_per_exp: 4
data_split_seed: 1024

# lower and upper bound of the 3D volume (in mm) anchored on animal
vmin: -100
vmax: 100
nvox: 80
interp: nearest

expval: True

### data augmentations ###      
medfilt_window: 30
rand_view_replace: True
n_rand_views: 8
mirror_augmentation: False
augment_hue: False
augment_brightness: False
augment_bright_val: 0.01

### model ###
# architecture options:
# "dannce"
# "compressed_dannce" (a channel-compressed version of the original dannce encoder-decoder)
net_type: "dannce" 
#n_channels_in: 1
#n_channels_in: 15

### train ###
batch_size: 4
epochs: 1200
save_period: 100

# Options:
# 'new': initializes and trains a network from scratch
# 'finetune': loads in pre-trained weights and fine-tuned from there
# 'continued': initializes a full model, including optimizer state, and continuous training from the last full model checkpoint
#train_mode: new
train_mode: finetune
dannce_finetune_weights: /home/jovyan/vast/ckapoor/keypoint-tracking/dannce-pytorch-custom/demo/single-mouse-markerless/DANNCE/train_lr_1e-2_batch_4_non-aligned_debug/checkpoint-epoch1200.pth # needed for 'finetune' and 'continued' mode

loss: 
  L1Loss:
    loss_weight: 1.0

metric: ['euclidean_distance_3D']

lr: 0.01
# one may find the available learning schedulers in 
# https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
#lr_scheduler:
#  type: StepLR
#  args: 
#    step_size: 100
#    gamma: 0.1

### prediction ###
max_num_samples: 1000
